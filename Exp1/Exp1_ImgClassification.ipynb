{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lightweight-choir",
   "metadata": {},
   "source": [
    "### 데이터 불러오기 + Resize하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-imperial",
   "metadata": {},
   "source": [
    "#### 라이브러리를 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "finished-council",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIL 라이브러리 import 완료!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob\n",
    "\n",
    "print(\"PIL 라이브러리 import 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-contest",
   "metadata": {},
   "source": [
    "#### 이미지를 불러와서 28x28 사이즈로 변경한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "formed-flush",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "가위 이미지 resize 완료!\n",
      "/aiffel\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "바위 이미지 resize 완료!\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "보 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def resize_images(img_path):\n",
    "    images=glob.glob(img_path + \"/*.jpg\")\n",
    "    \n",
    "    print(len(images), \" images to be resized.\")\n",
    "    \n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장\n",
    "    target_size=(28,28)\n",
    "    for img in images:\n",
    "        old_img=Image.open(img)\n",
    "        new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "        new_img.save(img, \"JPEG\")\n",
    "        \n",
    "    print(len(images), \" images resized.\")\n",
    "    \n",
    "# 가위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽음\n",
    "image_dir_path=os.getenv(\"HOME\")+\"/aiffel/workplace/Exp1/rock_scissor_paper/scissor\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"가위 이미지 resize 완료!\")\n",
    "print(os.getenv(\"HOME\"))\n",
    "\n",
    "# 바위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽음\n",
    "image_dir_path=os.getenv(\"HOME\")+\"/aiffel/workplace/Exp1/rock_scissor_paper/rock\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"바위 이미지 resize 완료!\")\n",
    "\n",
    "# 보 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽음\n",
    "image_dir_path=os.getenv(\"HOME\")+\"/aiffel/workplace/Exp1/rock_scissor_paper/paper\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"보 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-telling",
   "metadata": {},
   "source": [
    "#### load_data()라는 함수를 통해 데이터를 읽는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "wireless-payday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 300 입니다.\n",
      "x_train shape: (300, 28, 28, 3)\n",
      "y_train shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 가위바위보 이미지 개수 총합\n",
    "def load_data(img_path, number_of_data=300):\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/workplace/Exp1/rock_scissor_paper\"\n",
    "(x_train, y_train)=load_data(image_dir_path)\n",
    "# 입력은 0~1 사이의 값으로 정규화\n",
    "x_train_norm = x_train/255.0\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-blond",
   "metadata": {},
   "source": [
    "#### 이미지 부르기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "diverse-assessment",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[0])\n",
    "print('라벨: ', y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-nigeria",
   "metadata": {},
   "source": [
    "### 딥러닝 네트워크 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "mediterranean-canberra",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model에 추가된 Layer 개수:  7\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 16)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                25632     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 30,819\n",
      "Trainable params: 30,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(32, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "print('Model에 추가된 Layer 개수: ', len(model.layers)) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-teacher",
   "metadata": {},
   "source": [
    "### 딥러닝 네트워크 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "junior-whole",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Reshape - x_train_norm shape: (300, 28, 28, 3)\n",
      "Before Reshape - x_test_norm shape: (12, 28, 28, 3)\n",
      "After Reshape - x_train_reshaped shape: (300, 28, 28, 3)\n",
      "After Reshape - x_test_reshaped shape: (36, 28, 28, 1)\n",
      "Epoch 1/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1.1169 - accuracy: 0.2766\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1.0582 - accuracy: 0.3980\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.9882 - accuracy: 0.7774\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.8775 - accuracy: 0.7525\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.7246 - accuracy: 0.8261\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5793 - accuracy: 0.9138\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4264 - accuracy: 0.9590\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3021 - accuracy: 0.9691\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2128 - accuracy: 0.9662\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1292 - accuracy: 0.9987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f11db6ef310>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Before Reshape - x_train_norm shape: {}\".format(x_train_norm.shape))\n",
    "print(\"Before Reshape - x_test_norm shape: {}\".format(x_test_norm.shape))\n",
    "\n",
    "# 데이터갯수에 -1을 쓰면 reshape시 자동계산\n",
    "x_train_reshaped=x_train_norm.reshape( -1, 28, 28, 3)\n",
    "x_test_reshaped=x_test_norm.reshape( -1, 28, 28, 1)\n",
    "\n",
    "print(\"After Reshape - x_train_reshaped shape: {}\".format(x_train_reshaped.shape))\n",
    "print(\"After Reshape - x_test_reshaped shape: {}\".format(x_test_reshaped.shape))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_reshaped, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-spencer",
   "metadata": {},
   "source": [
    "### 테스트 데이터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fifteen-oregon",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4  images to be resized.\n",
      "4  images resized.\n",
      "가위 이미지 resize 완료!\n",
      "4  images to be resized.\n",
      "4  images resized.\n",
      "바위 이미지 resize 완료!\n",
      "4  images to be resized.\n",
      "4  images resized.\n",
      "보 이미지 resize 완료!\n",
      "학습데이터(x_test)의 이미지 개수는 12 입니다.\n",
      "x_test shape: (12, 28, 28, 3)\n",
      "y_test shape: (12,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def resize_images(img_path):\n",
    "    images=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "    print(len(images), \" images to be resized.\")\n",
    "\n",
    "    target_size=(28,28)\n",
    "    for img in images:\n",
    "        old_img=Image.open(img)\n",
    "        new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "        new_img.save(img, \"JPEG\")\n",
    "    \n",
    "    print(len(images), \" images resized.\")\n",
    "    \n",
    "# 가위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽음\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/workplace/Exp1/rock_scissor_paper/test/scissor\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"가위 이미지 resize 완료!\")\n",
    "\n",
    "# 바위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽음\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/workplace/Exp1/rock_scissor_paper/test/rock\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"바위 이미지 resize 완료!\")\n",
    "\n",
    "# 보 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽음\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/workplace/Exp1/rock_scissor_paper/test/paper\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"보 이미지 resize 완료!\")\n",
    "\n",
    "# x_test, y_test 만들기\n",
    "import numpy as np\n",
    "\n",
    "# 가위바위보 이미지 개수 총합\n",
    "def load_data(img_path, number_of_data=12):\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_test)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/workplace/Exp1/rock_scissor_paper/test\"\n",
    "(x_test, y_test)=load_data(image_dir_path)\n",
    "x_test_norm = x_test/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_test shape: {}\".format(x_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-cigarette",
   "metadata": {},
   "source": [
    "### model을 사용하여 accuracy 측정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "lightweight-milton",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_test_function.<locals>.test_function at 0x7f11db6820e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 - 0s - loss: 152.8927 - accuracy: 0.5000\n",
      "test_loss: 152.89271545410156 \n",
      "test_accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=2)\n",
    "\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-weekly",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
